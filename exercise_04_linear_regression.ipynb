{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Task: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a code skeleton for performing linear regression. \n",
    "Your task is to complete the functions where required. \n",
    "You are only allowed to use built-in Python functions, as well as any `numpy` functions. No other libraries / imports are allowed.\n",
    "\n",
    "In the beginning of every function there is docstring which specifies the input and and expected output.\n",
    "Write your code in a way that adheres to it.\n",
    "You may only use plain python and anything that we imported for you above such as numpy functions (i.e. no scikit-learn classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the results to PDF\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook (`Kernel -> Restart & Run All`)\n",
    "2. Export/download the notebook as PDF (`File -> Download as -> PDF via LaTeX (.pdf)`)\n",
    "3. Concatenate your solutions for other tasks with the output of Step 2. On Linux you can simply use `pdfunite`, there are similar tools for other platforms too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "**Make sure** you are using `nbconvert` **Version 5.5 or later** by running `jupyter nbconvert --version`. Older versions clip lines that exceed page width, which makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will work with the Boston Housing Dataset.\n",
    "The data consists of 506 samples. Each sample represents a district in the city of Boston and has 13 features, such as crime rate or taxation level. The regression target is the median house price in the given district (in $1000's).\n",
    "\n",
    "More details can be found here: http://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n",
      "(506, 14)\n",
      "(404, 14)\n",
      "(102, 14)\n",
      "[1 1]\n",
      "[[0 2]\n",
      " [1 3]\n",
      " [3 3]]\n",
      "(2,)\n",
      "(3, 2)\n",
      "[3 3]\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "X , y = load_boston(return_X_y=True)\n",
    "print(X.shape) # (506,13)\n",
    "print(y.shape) # (506)\n",
    "\n",
    "# Add a vector of ones to the data matrix to absorb the bias term\n",
    "# (Recall slide #7 from the lecture)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "# np.ones([X.shape[0], 1]) -> (506*1) add a column vector consist of ones\n",
    "print(X.shape) # (506, 14)\n",
    "# From now on, D refers to the number of features in the AUGMENTED dataset (i.e. including the dummy '1' feature for the absorbed bias term)\n",
    "\n",
    "# Split into train and test\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "print(X_train.shape) #506*0.8 = 404\n",
    "print(X_test.shape) #506*0.2 = 102\n",
    "\n",
    "\n",
    "a= np.array([1, 1])\n",
    "print(a)\n",
    "b= np.array([[0, 1, 3],[2, 3, 3]])\n",
    "print(np.transpose(b))\n",
    "print(a.shape)\n",
    "print(np.transpose(b).shape)\n",
    "\n",
    "print(np.add(a, 2))\n",
    "print(np.sum(a))\n",
    "print(np.matmul(a, np.transpose(np.array([2, 2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Fit standard linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_least_squares(X, y):\n",
    "    \"\"\"Fit ordinary least squares model to the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Regression targets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "   \n",
    "    \n",
    "    result = np.linalg.inv(np.matmul( np.transpose(X), X )) #(X^T * X)^-1\n",
    "    result = np.matmul(result, np.transpose(X)) #(X^T * X)^-1 * X^T\n",
    "    result = np.matmul(result, y)  #(X^T * X)^-1 * X^T *y\n",
    "    # X^T * X * w = X^T * y, Ax=b\n",
    "    # X^T * X <=> A\n",
    "    # X^T * y <=> b\n",
    "    # -> w = np.linalg.solve(X.T.dot(X), X.T.dot(y)) !!!!!\n",
    "    \n",
    "    # np.matmul( np.transpose(X), X ) => X.T.dot(X)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fit ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, y, reg_strength):\n",
    "    \"\"\"Fit ridge regression model to the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Regression targets.\n",
    "    reg_strength : float\n",
    "        L2 regularization strength (denoted by lambda in the lecture)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" \n",
    "    ||w||_2^2 = w_0^2+....w_M^2\n",
    "    \n",
    "    d(w_0^2+....w_M^2)/d(w) = [2w_1, 2w_2, ..., 2w_n]^T  = w(vector)\n",
    "    \n",
    "    x^Txw-x^Ty+lambda*w=0\n",
    "    (x^x+lambda)w=x^Ty\n",
    "    \n",
    "    !!!!!!!!CHECK!!!!!!!can be problematic\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    #result = np.linalg.inv( np.add(np.matmul(np.transpose(X),X), reg_strength) ) #(X^T * X + lambda)^-1\n",
    "    #result = np.matmul(result, np.transpose(X)) #(X^T * X + lambda)^-1 * X^T\n",
    "    #result = np.matmul(result, y) \n",
    "    \n",
    "    #(X^T * X + lambda*I)*w = X^T *y\n",
    "    \n",
    "    I_size = X.shape[1]\n",
    "    Iden = np.identity(I_size)\n",
    "    \n",
    "    result = np.linalg.solve( X.T.dot(X) + reg_strength*Iden, X.T.dot(y)  )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Generate predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_model(X, w):\n",
    "    \"\"\"Generate predictions for the given samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array, shape [N]\n",
    "        Predicted regression targets for the input data.\n",
    "        \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    return np.matmul(X,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Compute mean squared error between true and predicted regression targets.\n",
    "    \n",
    "    Reference: `https://en.wikipedia.org/wiki/Mean_squared_error`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array\n",
    "        True regression targets.\n",
    "    y_pred : array\n",
    "        Predicted regression targets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mse : float\n",
    "        Mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    num = y_true.shape[0]\n",
    "    \n",
    "    result = (np.sum( np.power(np.subtract(y_true, y_pred),2)  ))/num\n",
    "    \n",
    "    # d = y_true-y_pred\n",
    "    # length = len(y_true)\n",
    "    # return 1/length * d.dot(d)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference implementation produces \n",
    "* MSE for Least squares $\\approx$ **23.96**\n",
    "* MSE for Ridge regression $\\approx$ **21.03**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 1\\%$) different from the reference soultion due to numerical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Least squares = 23.96457138495312\n",
      "MSE for Ridge regression = 21.034931215917844\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "np.random.seed(1234)\n",
    "X , y = load_boston(return_X_y=True)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "# Ordinary least squares regression\n",
    "w_ls = fit_least_squares(X_train, y_train)\n",
    "y_pred_ls = predict_linear_model(X_test, w_ls)\n",
    "mse_ls = mean_squared_error(y_test, y_pred_ls)\n",
    "print('MSE for Least squares = {0}'.format(mse_ls))\n",
    "\n",
    "# Ridge regression\n",
    "reg_strength = 1\n",
    "w_ridge = fit_ridge(X_train, y_train, reg_strength)\n",
    "y_pred_ridge = predict_linear_model(X_test, w_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print('MSE for Ridge regression = {0}'.format(mse_ridge))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}